{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkPAB5j7crrk"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets accelerate bitsandbytes\n",
        "!pip install unsloth\n",
        "!pip install peft\n",
        "!pip install torch\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBKg2vLccyBg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import Dataset\n",
        "from unsloth import FastLanguageModel\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WL5nWmJic19b"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rn9nyb17c45a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuMXVfvic-Pm"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel(\"en-tel-colloquial.xlsx\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "df = df.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
        "df.columns = df.columns.str.strip()\n",
        "df.dropna(inplace=True)  # Drop NaNs to prevent errors\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Dataset columns: {df.columns.tolist()}\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcCZ69yEc-6t"
      },
      "outputs": [],
      "source": [
        "df['text'] = df.apply(lambda row: f\"### Human:Translate to Telugu colloquial: {row['Human']}\\n### Assistant: {row['Assistant']}\", axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tw2pozLKdCDh"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "df['text'] = df['text'].apply(normalize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhhh9uNMdEpb"
      },
      "outputs": [],
      "source": [
        "# Step 3: Convert the DataFrame into a Dataset\n",
        "dataset = Dataset.from_pandas(df[['text']])  # Use only the 'text' column\n",
        "dataset = dataset.shuffle(seed=42)\n",
        "split_dataset = dataset.train_test_split(test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTqloIeHdHxp"
      },
      "outputs": [],
      "source": [
        "# Preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    model_inputs = tokenizer(\n",
        "        examples['text'],\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    model_inputs['labels'] = model_inputs['input_ids']\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYgb9AS9dLQn"
      },
      "outputs": [],
      "source": [
        "# Hugging Face User Name\n",
        "hugging_face_user_name=\"xxxx\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yU66Eb8ydP_d"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login('xxxxx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwL6akPzdTNi"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t64useIVdaNs"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load base model again\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"unsloth/llama-3.2-3b-instruct-unsloth-bnb-4bit\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load the previously fine-tuned adapter\n",
        "model = PeftModel.from_pretrained(base_model, \"sril32996/en-tel\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/llama-3.2-3b-instruct-unsloth-bnb-4bit\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1qUCJzyeOO7"
      },
      "outputs": [],
      "source": [
        "model.enable_input_require_grads()\n",
        "model.gradient_checkpointing_enable()\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5249iX4eypg"
      },
      "outputs": [],
      "source": [
        "tokenized_train = split_dataset['train'].map(preprocess_function, remove_columns=['text'], batched=True)\n",
        "tokenized_val = split_dataset['test'].map(preprocess_function, remove_columns=['text'], batched=True)\n",
        "\n",
        "# Convert to PyTorch format\n",
        "tokenized_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "tokenized_val.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xA-dGshle3Js"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./fine_tuned_llama\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=7,\n",
        "    learning_rate=3e-4,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=8,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=8,\n",
        "    save_total_limit=2,  # Prevent excessive checkpoints\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=True,    # Enable fp16 precision\n",
        "    bf16=False,\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=f\"{hugging_face_user_name}/en-tel\",\n",
        "    gradient_accumulation_steps=8,\n",
        "    warmup_steps=100,\n",
        "    report_to=[\"none\"],\n",
        "    optim=\"adamw_torch\",\n",
        "    dataloader_pin_memory=False,\n",
        "    torch_compile=False,\n",
        "    gradient_checkpointing=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHEO4Fi8fEdr"
      },
      "outputs": [],
      "source": [
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(model_name)\n",
        "tokenizer.save_pretrained(model_name)\n",
        "\n",
        "\n",
        "model.push_to_hub(MODEL_NAME, token=hf_token)\n",
        "tokenizer.push_to_hub(MODEL_NAME, token=hf_token)\n",
        "\n",
        "# Save and push to hub\n",
        "print(\"Saving model and pushing to Hugging Face Hub...\")\n",
        "trainer.save_model()\n",
        "trainer.push_to_hub()\n",
        "\n",
        "# Prepare model for inference\n",
        "print(\"Preparing model for inference...\")\n",
        "model = FastLanguageModel.for_inference(model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
